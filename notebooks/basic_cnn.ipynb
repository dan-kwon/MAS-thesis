{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7fcc38be02e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from custom_models import cnn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision.transforms import v2\n",
    "from tempfile import TemporaryDirectory\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data transformations to loop through\n",
    "minimal_transforms = {\n",
    "    'synthetic_train': transforms.Compose([\n",
    "        v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n",
    "        v2.Grayscale(num_output_channels=1),\n",
    "        v2.Normalize([0.5, ],[0.5, ]),\n",
    "        v2.Resize((128, 128))\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n",
    "        v2.Grayscale(num_output_channels=1),\n",
    "        v2.Normalize([0.5, ],[0.5, ]),\n",
    "        v2.Resize((128, 128))\n",
    "    ]),\n",
    "}\n",
    "\n",
    "basic_transforms = {\n",
    "    'synthetic_train': transforms.Compose([\n",
    "        v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n",
    "        v2.Grayscale(num_output_channels=1),\n",
    "        v2.Normalize([0.5, ],[0.5, ]),\n",
    "        v2.RandomHorizontalFlip(p=0.5),\n",
    "        v2.Resize((128, 128))\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n",
    "        v2.Grayscale(num_output_channels=1),\n",
    "        v2.Normalize([0.5, ],[0.5, ]),\n",
    "        v2.Resize((128, 128))\n",
    "    ]),\n",
    "}\n",
    "\n",
    "auto_transforms = {\n",
    "    'synthetic_train': transforms.Compose([\n",
    "        v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n",
    "        v2.Grayscale(num_output_channels=1),\n",
    "        v2.Normalize([0.5, ],[0.5, ]),\n",
    "        v2.AutoAugment(policy=v2.AutoAugmentPolicy.IMAGENET),\n",
    "        v2.Resize((128, 128))\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n",
    "        v2.Grayscale(num_output_channels=1),\n",
    "        v2.Normalize([0.5, ],[0.5, ]),\n",
    "        v2.Resize((128, 128))\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Training Set with Real/Synthetic Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeSyntheticTrain(train_directory, synthetic_train_directory, train_percentage, synthetic_percentage):\n",
    "\n",
    "    # Remove any existing images in directory\n",
    "    try:\n",
    "        shutil.rmtree(synthetic_train_directory)\n",
    "    except:\n",
    "        print(\"directory does not exist\")\n",
    "\n",
    "    # Loop through subfolders, generate synthetic images\n",
    "    subfolders = [f for f in os.listdir(train_directory)]\n",
    "\n",
    "    for s in subfolders:\n",
    "        # for each subfolder in the train directory, make the same in the synthetic train directory\n",
    "        os.makedirs(f\"{synthetic_train_directory}/{s}\", exist_ok=True)\n",
    "        \n",
    "        # get a random sample from each subfolder\n",
    "        subfolder_path = f\"{train_directory}/{s}\"\n",
    "        files = os.listdir(subfolder_path)\n",
    "        sample_files = random.sample(files, round(len(files)*train_percentage))\n",
    "        \n",
    "        # create synthetic sample based on sampled original images\n",
    "        synthetic_subfolder_path = subfolder_path.replace('train','synthetic')\n",
    "        synthetic_files = [f for f in os.listdir(synthetic_subfolder_path) if int(f.replace('.png','').split('_')[1]) in [int(f.replace('.png','').split('_')[1]) for f in sample_files]]\n",
    "        synthetic_sample_files = random.sample(synthetic_files, round(len(files)*synthetic_percentage))\n",
    "        \n",
    "        # Move sample files to synthetic directory\n",
    "        for f in sample_files:\n",
    "            \n",
    "            image_path = f\"{subfolder_path}/{f}\"\n",
    "            destination_directory = f\"{synthetic_train_directory}/{s}/\"\n",
    "            shutil.copyfile(image_path, destination_directory + image_path.split('/')[-1])\n",
    "\n",
    "        # Move synthetic sample files to synthetic directory\n",
    "        for f in synthetic_sample_files:\n",
    "\n",
    "            image_path = f\"{synthetic_subfolder_path}/{f}\"\n",
    "            destination_directory = f\"{synthetic_train_directory}/{s}/\"\n",
    "            shutil.copyfile(image_path, destination_directory + image_path.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_dir, data_sets, data_transforms):\n",
    "    \n",
    "    image_datasets = {\n",
    "        x: datasets.ImageFolder(\n",
    "            os.path.join(data_dir, x),\n",
    "            data_transforms[x]\n",
    "        )\n",
    "        for x in data_sets\n",
    "    }\n",
    "\n",
    "    dataloaders = {\n",
    "        x: DataLoader(\n",
    "            image_datasets[x], \n",
    "            batch_size=4,\n",
    "            shuffle=True, \n",
    "            num_workers=4\n",
    "        )\n",
    "        for x in data_sets\n",
    "    }\n",
    "\n",
    "    dataset_sizes = {\n",
    "        x: len(image_datasets[x]) \n",
    "        for x in data_sets\n",
    "    }\n",
    "\n",
    "    class_names = image_datasets['synthetic_train'].classes\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    return image_datasets, dataloaders, dataset_sizes, class_names, device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, dataloaders, dataset_sizes, num_epochs=10):\n",
    "    since = time.time()\n",
    "\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        \n",
    "        best_test_loss = float('inf')\n",
    "        patience = 2  # Number of epochs to wait for improvement before stopping\n",
    "        test_losses = []\n",
    "        train_losses = []\n",
    "        test_acc = []\n",
    "        train_acc = []\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['synthetic_train', 'test']:\n",
    "                if phase == 'synthetic_train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'synthetic_train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'synthetic_train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double().item() / dataset_sizes[phase]\n",
    "\n",
    "                if phase =='synthetic_train':\n",
    "                    train_losses.append(epoch_loss)\n",
    "                    train_acc.append(epoch_acc)\n",
    "                else:\n",
    "                    test_losses.append(epoch_loss)\n",
    "                    test_acc.append(epoch_acc)\n",
    "                    \n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "                # deep copy the model\n",
    "                if phase == 'test' and epoch_loss <= best_test_loss:\n",
    "                    best_test_loss = epoch_loss\n",
    "                    patience_counter = 0  # Reset counter\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "                elif phase == 'test' and epoch_loss > best_test_loss:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                    # Early stopping check\n",
    "                    if patience_counter >= patience:\n",
    "                        print(\"Stopping early due to no improvement in validation loss.\")\n",
    "                        break\n",
    "\n",
    "        # store results in dataframe\n",
    "        dat = {\n",
    "            \"epoch\": range(len(test_losses)),\n",
    "            \"test_losses\": test_losses,\n",
    "            \"train_losses\": train_losses,\n",
    "            \"test_accuracies\": test_acc,\n",
    "            \"train_accuracies\": train_acc\n",
    "        }\n",
    "\n",
    "        result = pd.DataFrame(data=dat)\n",
    "        print()\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best test Loss: {best_test_loss:4f}')\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path, weights_only=True))\n",
    "    return result, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop through different training scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danielkwon/MAS-thesis/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "synthetic_train Loss: 0.8610 Acc: 0.5859\n",
      "test Loss: 0.5707 Acc: 0.7523\n",
      "Epoch 1/24\n",
      "----------\n",
      "synthetic_train Loss: 0.3208 Acc: 0.8791\n",
      "test Loss: 0.3198 Acc: 0.8859\n",
      "Epoch 2/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0878 Acc: 0.9662\n",
      "test Loss: 0.2020 Acc: 0.9359\n",
      "Epoch 3/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0478 Acc: 0.9826\n",
      "test Loss: 0.1210 Acc: 0.9656\n",
      "Epoch 4/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0517 Acc: 0.9803\n",
      "test Loss: 0.1440 Acc: 0.9563\n",
      "Epoch 5/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0303 Acc: 0.9902\n",
      "test Loss: 0.1397 Acc: 0.9711\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 6/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0249 Acc: 0.9930\n",
      "test Loss: 0.1169 Acc: 0.9750\n",
      "Epoch 7/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0265 Acc: 0.9906\n",
      "test Loss: 0.1289 Acc: 0.9742\n",
      "Epoch 8/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0240 Acc: 0.9926\n",
      "test Loss: 0.1910 Acc: 0.9664\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 9/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0303 Acc: 0.9922\n",
      "test Loss: 0.2194 Acc: 0.9336\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 10/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0194 Acc: 0.9945\n",
      "test Loss: 0.1113 Acc: 0.9758\n",
      "Epoch 11/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0026 Acc: 0.9994\n",
      "test Loss: 0.1268 Acc: 0.9820\n",
      "Epoch 12/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1300 Acc: 0.9852\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 13/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1320 Acc: 0.9844\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 14/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1355 Acc: 0.9852\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 15/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1381 Acc: 0.9852\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 16/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1418 Acc: 0.9875\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 17/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1476 Acc: 0.9875\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 18/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1506 Acc: 0.9875\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 19/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1549 Acc: 0.9875\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 20/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1537 Acc: 0.9875\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 21/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1593 Acc: 0.9875\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 22/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1682 Acc: 0.9875\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 23/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0719 Acc: 0.9871\n",
      "test Loss: 0.1678 Acc: 0.9656\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 24/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0177 Acc: 0.9953\n",
      "test Loss: 0.2113 Acc: 0.9492\n",
      "Stopping early due to no improvement in validation loss.\n",
      "\n",
      "Training complete in 8m 25s\n",
      "Best test Loss: 0.111342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danielkwon/MAS-thesis/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "synthetic_train Loss: 0.8388 Acc: 0.6088\n",
      "test Loss: 0.6176 Acc: 0.7484\n",
      "Epoch 1/24\n",
      "----------\n",
      "synthetic_train Loss: 0.2762 Acc: 0.8934\n",
      "test Loss: 0.2574 Acc: 0.9062\n",
      "Epoch 2/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0557 Acc: 0.9818\n",
      "test Loss: 0.1999 Acc: 0.9344\n",
      "Epoch 3/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0521 Acc: 0.9834\n",
      "test Loss: 0.1968 Acc: 0.9445\n",
      "Epoch 4/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0355 Acc: 0.9900\n",
      "test Loss: 0.1175 Acc: 0.9633\n",
      "Epoch 5/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0361 Acc: 0.9879\n",
      "test Loss: 0.1181 Acc: 0.9672\n",
      "Epoch 6/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0045 Acc: 0.9984\n",
      "test Loss: 0.1862 Acc: 0.9445\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 7/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0367 Acc: 0.9889\n",
      "test Loss: 0.1476 Acc: 0.9664\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 8/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0301 Acc: 0.9904\n",
      "test Loss: 0.1590 Acc: 0.9523\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 9/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0215 Acc: 0.9934\n",
      "test Loss: 0.0984 Acc: 0.9688\n",
      "Epoch 10/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0118 Acc: 0.9963\n",
      "test Loss: 0.1564 Acc: 0.9672\n",
      "Epoch 11/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0051 Acc: 0.9984\n",
      "test Loss: 0.1007 Acc: 0.9812\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 12/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1401 Acc: 0.9828\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 13/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1475 Acc: 0.9812\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 14/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1558 Acc: 0.9805\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 15/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1603 Acc: 0.9797\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 16/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1475 Acc: 0.9820\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 17/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1654 Acc: 0.9812\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 18/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1623 Acc: 0.9812\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 19/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1636 Acc: 0.9812\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 20/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1742 Acc: 0.9820\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 21/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1690 Acc: 0.9828\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 22/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1758 Acc: 0.9812\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 23/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1701 Acc: 0.9828\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 24/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1731 Acc: 0.9836\n",
      "Stopping early due to no improvement in validation loss.\n",
      "\n",
      "Training complete in 8m 23s\n",
      "Best test Loss: 0.098441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danielkwon/MAS-thesis/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "synthetic_train Loss: 0.8355 Acc: 0.5998\n",
      "test Loss: 0.5776 Acc: 0.7586\n",
      "Epoch 1/24\n",
      "----------\n",
      "synthetic_train Loss: 0.3195 Acc: 0.8727\n",
      "test Loss: 0.1936 Acc: 0.9328\n",
      "Epoch 2/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0868 Acc: 0.9695\n",
      "test Loss: 0.0985 Acc: 0.9664\n",
      "Epoch 3/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0557 Acc: 0.9826\n",
      "test Loss: 0.1708 Acc: 0.9445\n",
      "Epoch 4/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0307 Acc: 0.9908\n",
      "test Loss: 0.1906 Acc: 0.9500\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 5/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0360 Acc: 0.9869\n",
      "test Loss: 0.1750 Acc: 0.9305\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 6/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0363 Acc: 0.9873\n",
      "test Loss: 0.2382 Acc: 0.9328\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 7/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0305 Acc: 0.9898\n",
      "test Loss: 0.1845 Acc: 0.9492\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 8/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0226 Acc: 0.9934\n",
      "test Loss: 0.1063 Acc: 0.9711\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 9/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0217 Acc: 0.9938\n",
      "test Loss: 0.1800 Acc: 0.9469\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 10/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0289 Acc: 0.9904\n",
      "test Loss: 0.1150 Acc: 0.9695\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 11/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0159 Acc: 0.9943\n",
      "test Loss: 0.2001 Acc: 0.9539\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 12/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0182 Acc: 0.9934\n",
      "test Loss: 0.1844 Acc: 0.9469\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 13/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0181 Acc: 0.9949\n",
      "test Loss: 0.1175 Acc: 0.9633\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 14/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0189 Acc: 0.9941\n",
      "test Loss: 0.0967 Acc: 0.9766\n",
      "Epoch 15/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1016 Acc: 0.9773\n",
      "Epoch 16/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1024 Acc: 0.9781\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 17/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1089 Acc: 0.9781\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 18/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1084 Acc: 0.9781\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 19/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1117 Acc: 0.9789\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 20/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1067 Acc: 0.9812\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 21/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1075 Acc: 0.9820\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 22/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.1085 Acc: 0.9828\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 23/24\n",
      "----------\n",
      "synthetic_train Loss: 0.0000 Acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "num_sims = 10\n",
    "train_percs = [1.0, 0.8, 0.8]\n",
    "synth_percs = [0.0, 0.0, 0.2]\n",
    "transforms = {\n",
    "    'minimal': minimal_transforms,\n",
    "    #'basic': basic_transforms,\n",
    "    #'auto': auto_transforms\n",
    "}\n",
    "\n",
    "for train_percentage, synthetic_percentage in zip(train_percs, synth_percs):\n",
    "    for active_transform in transforms.keys():\n",
    "        df_all_results = pd.DataFrame()\n",
    "        for n in range(num_sims):\n",
    "    \n",
    "            # make the synthetic training dataset\n",
    "            makeSyntheticTrain(\n",
    "                train_directory='../data/alzheimer_mri/train',\n",
    "                synthetic_train_directory='../data/alzheimer_mri/synthetic_train', \n",
    "                train_percentage=train_percentage, \n",
    "                synthetic_percentage=synthetic_percentage\n",
    "            )\n",
    "\n",
    "            # get and load datasets\n",
    "            data_dir = '../data/alzheimer_mri'\n",
    "            data_sets = ['synthetic_train','test']\n",
    "            image_datasets, dataloaders, dataset_sizes, class_names, device = get_data(\n",
    "                data_dir=data_dir, \n",
    "                data_sets=data_sets, \n",
    "                data_transforms=transforms.get(active_transform)\n",
    "            )\n",
    "\n",
    "            # instantiate model\n",
    "            model = cnn.CNN(in_channels=1, num_classes=4)\n",
    "\n",
    "            # Set the size of each output sample to nn.Linear(num_ftrs, len(class_names))\n",
    "            #num_ftrs = 4 #model.fc.in_features\n",
    "            #model.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "            model = model.to(device)\n",
    "\n",
    "            # train the model\n",
    "            df_results,_ = model = train_model(\n",
    "                model=model, \n",
    "                criterion = nn.CrossEntropyLoss(),\n",
    "                optimizer = optim.Adam(model.parameters(), lr=0.001), \n",
    "                dataloaders=dataloaders,\n",
    "                dataset_sizes=dataset_sizes,\n",
    "                num_epochs=num_epochs\n",
    "            )\n",
    "\n",
    "            df_results['train_percentage'] = train_percentage\n",
    "            df_results['synth_percentage'] = synthetic_percentage\n",
    "            df_results['transform'] = active_transform\n",
    "            df_results['sim_num'] = n\n",
    "            df_results['category'] = df_results.apply(lambda row: row['transform']+'_'+str(row['train_percentage'])+'_'+str(row['synth_percentage']), axis=1)\n",
    "            df_all_results = pd.concat([df_all_results, df_results],ignore_index=True)\n",
    "        df_all_results.to_csv('../results/results_cnn_'+active_transform+'_'+str(train_percentage)+'_'+str(synthetic_percentage)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
