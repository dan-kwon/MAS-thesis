{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7f45bbfe57b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from custom_models import cnn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision.transforms import v2\n",
    "from tempfile import TemporaryDirectory\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data transformations to loop through\n",
    "minimal_transforms = {\n",
    "    'synthetic_train': transforms.Compose([\n",
    "        v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n",
    "        v2.Grayscale(num_output_channels=1),\n",
    "        v2.Normalize([0.5, ],[0.5, ]),\n",
    "        v2.Resize((128, 128))\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n",
    "        v2.Grayscale(num_output_channels=1),\n",
    "        v2.Normalize([0.5, ],[0.5, ]),\n",
    "        v2.Resize((128, 128))\n",
    "    ]),\n",
    "}\n",
    "\n",
    "basic_transforms = {\n",
    "    'synthetic_train': transforms.Compose([\n",
    "        v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n",
    "        v2.Grayscale(num_output_channels=1),\n",
    "        v2.Normalize([0.5, ],[0.5, ]),\n",
    "        v2.RandomHorizontalFlip(p=0.5),\n",
    "        v2.Resize((128, 128))\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n",
    "        v2.Grayscale(num_output_channels=1),\n",
    "        v2.Normalize([0.5, ],[0.5, ]),\n",
    "        v2.Resize((128, 128))\n",
    "    ]),\n",
    "}\n",
    "\n",
    "auto_transforms = {\n",
    "    'synthetic_train': transforms.Compose([\n",
    "        v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n",
    "        v2.Grayscale(num_output_channels=1),\n",
    "        v2.Normalize([0.5, ],[0.5, ]),\n",
    "        v2.AutoAugment(policy=v2.AutoAugmentPolicy.IMAGENET),\n",
    "        v2.Resize((128, 128))\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n",
    "        v2.Grayscale(num_output_channels=1),\n",
    "        v2.Normalize([0.5, ],[0.5, ]),\n",
    "        v2.Resize((128, 128))\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Training Set with Real/Synthetic Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeSyntheticTrain(train_directory, synthetic_train_directory, train_dict, synthetic_dict):\n",
    "\n",
    "    # Remove any existing images in directory\n",
    "    try:\n",
    "        shutil.rmtree(synthetic_train_directory)\n",
    "    except:\n",
    "        print(\"directory does not exist\")\n",
    "\n",
    "    # Loop through subfolders, generate synthetic images\n",
    "    subfolders = [f for f in os.listdir(train_directory)]\n",
    "\n",
    "    for s in subfolders:\n",
    "        # for each subfolder in the train directory, make the same in the synthetic train directory\n",
    "        os.makedirs(f\"{synthetic_train_directory}/{s}\", exist_ok=True)\n",
    "        \n",
    "        # get a random sample from each subfolder\n",
    "        subfolder_path = f\"{train_directory}/{s}\"\n",
    "        files = os.listdir(subfolder_path)\n",
    "        sample_files = random.sample(files, round(len(files)*train_dict[s]))\n",
    "        \n",
    "        # create synthetic sample based on sampled original images\n",
    "        synthetic_subfolder_path = subfolder_path.replace('train','synthetic')\n",
    "        synthetic_files = [f for f in os.listdir(synthetic_subfolder_path) if int(f.replace('.png','').split('_')[1]) in [int(f.replace('.png','').split('_')[1]) for f in sample_files]]\n",
    "        synthetic_sample_files = random.sample(synthetic_files, round(len(files)*synthetic_dict[s]))\n",
    "        \n",
    "        # Move sample files to synthetic directory\n",
    "        for f in sample_files:\n",
    "            \n",
    "            image_path = f\"{subfolder_path}/{f}\"\n",
    "            destination_directory = f\"{synthetic_train_directory}/{s}/\"\n",
    "            shutil.copyfile(image_path, destination_directory + image_path.split('/')[-1])\n",
    "\n",
    "        # Move synthetic sample files to synthetic directory\n",
    "        for f in synthetic_sample_files:\n",
    "\n",
    "            image_path = f\"{synthetic_subfolder_path}/{f}\"\n",
    "            destination_directory = f\"{synthetic_train_directory}/{s}/\"\n",
    "            shutil.copyfile(image_path, destination_directory + image_path.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_dir, data_sets, data_transforms):\n",
    "    \n",
    "    image_datasets = {\n",
    "        x: datasets.ImageFolder(\n",
    "            os.path.join(data_dir, x),\n",
    "            data_transforms[x]\n",
    "        )\n",
    "        for x in data_sets\n",
    "    }\n",
    "\n",
    "    dataloaders = {\n",
    "        x: DataLoader(\n",
    "            image_datasets[x],\n",
    "            batch_size=16,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        for x in data_sets\n",
    "    }\n",
    "\n",
    "    dataset_sizes = {\n",
    "        x: len(image_datasets[x]) \n",
    "        for x in data_sets\n",
    "    }\n",
    "\n",
    "    class_names = image_datasets['synthetic_train'].classes\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    return image_datasets, dataloaders, dataset_sizes, class_names, device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, dataloaders, dataset_sizes, num_epochs=10):\n",
    "    since = time.time()\n",
    "\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        \n",
    "        best_test_loss = float('inf')\n",
    "        patience = 2  # Number of epochs to wait for improvement before stopping\n",
    "        test_losses = []\n",
    "        train_losses = []\n",
    "        test_acc = []\n",
    "        train_acc = []\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['synthetic_train', 'test']:\n",
    "                if phase == 'synthetic_train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'synthetic_train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'synthetic_train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double().item() / dataset_sizes[phase]\n",
    "\n",
    "                if phase =='synthetic_train':\n",
    "                    train_losses.append(epoch_loss)\n",
    "                    train_acc.append(epoch_acc)\n",
    "                else:\n",
    "                    test_losses.append(epoch_loss)\n",
    "                    test_acc.append(epoch_acc)\n",
    "                    \n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "                # deep copy the model\n",
    "                if phase == 'test' and epoch_loss <= best_test_loss:\n",
    "                    best_test_loss = epoch_loss\n",
    "                    patience_counter = 0  # Reset counter\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "                elif phase == 'test' and epoch_loss > best_test_loss:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                    # Early stopping check\n",
    "                    if patience_counter >= patience:\n",
    "                        print(\"Stopping early due to no improvement in validation loss.\")\n",
    "                        break\n",
    "\n",
    "        # store results in dataframe\n",
    "        dat = {\n",
    "            \"epoch\": range(len(test_losses)),\n",
    "            \"test_losses\": test_losses,\n",
    "            \"train_losses\": train_losses,\n",
    "            \"test_accuracies\": test_acc,\n",
    "            \"train_accuracies\": train_acc\n",
    "        }\n",
    "\n",
    "        result = pd.DataFrame(data=dat)\n",
    "        print()\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best test Loss: {best_test_loss:4f}')\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path, weights_only=True))\n",
    "    return result, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop through different training scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n",
      "synthetic_train Loss: 1.0555 Acc: 0.4924\n",
      "test Loss: 0.9887 Acc: 0.4953\n",
      "Epoch 1/14\n",
      "----------\n",
      "synthetic_train Loss: 1.0240 Acc: 0.5017\n",
      "test Loss: 0.9167 Acc: 0.5320\n",
      "Epoch 2/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9936 Acc: 0.5186\n",
      "test Loss: 1.0082 Acc: 0.5117\n",
      "Epoch 3/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9636 Acc: 0.5330\n",
      "test Loss: 0.8527 Acc: 0.5859\n",
      "Epoch 4/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9138 Acc: 0.5598\n",
      "test Loss: 0.7503 Acc: 0.6602\n",
      "Epoch 5/14\n",
      "----------\n",
      "synthetic_train Loss: 0.8566 Acc: 0.5811\n",
      "test Loss: 0.7077 Acc: 0.6711\n",
      "Epoch 6/14\n",
      "----------\n",
      "synthetic_train Loss: 0.7896 Acc: 0.6287\n",
      "test Loss: 0.5959 Acc: 0.7609\n",
      "Epoch 7/14\n",
      "----------\n",
      "synthetic_train Loss: 0.7199 Acc: 0.6677\n",
      "test Loss: 0.5371 Acc: 0.7969\n",
      "Epoch 8/14\n",
      "----------\n",
      "synthetic_train Loss: 0.6659 Acc: 0.6936\n",
      "test Loss: 0.4870 Acc: 0.8180\n",
      "Epoch 9/14\n",
      "----------\n",
      "synthetic_train Loss: 0.6120 Acc: 0.7329\n",
      "test Loss: 0.4092 Acc: 0.8578\n",
      "Epoch 10/14\n",
      "----------\n",
      "synthetic_train Loss: 0.5828 Acc: 0.7380\n",
      "test Loss: 0.3070 Acc: 0.8891\n",
      "Epoch 11/14\n",
      "----------\n",
      "synthetic_train Loss: 0.5325 Acc: 0.7725\n",
      "test Loss: 0.3436 Acc: 0.8695\n",
      "Epoch 12/14\n",
      "----------\n",
      "synthetic_train Loss: 0.4968 Acc: 0.7820\n",
      "test Loss: 0.2806 Acc: 0.9109\n",
      "Epoch 13/14\n",
      "----------\n",
      "synthetic_train Loss: 0.4582 Acc: 0.8081\n",
      "test Loss: 0.3810 Acc: 0.8852\n",
      "Epoch 14/14\n",
      "----------\n",
      "synthetic_train Loss: 0.4546 Acc: 0.8015\n",
      "test Loss: 0.2626 Acc: 0.9133\n",
      "\n",
      "Training complete in 2m 39s\n",
      "Best test Loss: 0.262625\n",
      "Epoch 0/14\n",
      "----------\n",
      "synthetic_train Loss: 1.0580 Acc: 0.4795\n",
      "test Loss: 1.0066 Acc: 0.4953\n",
      "Epoch 1/14\n",
      "----------\n",
      "synthetic_train Loss: 1.0236 Acc: 0.5081\n",
      "test Loss: 0.9350 Acc: 0.5641\n",
      "Epoch 2/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9930 Acc: 0.5244\n",
      "test Loss: 0.9028 Acc: 0.5805\n",
      "Epoch 3/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9661 Acc: 0.5337\n",
      "test Loss: 0.8951 Acc: 0.5914\n",
      "Epoch 4/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9412 Acc: 0.5378\n",
      "test Loss: 0.8464 Acc: 0.5969\n",
      "Epoch 5/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9129 Acc: 0.5591\n",
      "test Loss: 0.7830 Acc: 0.6352\n",
      "Epoch 6/14\n",
      "----------\n",
      "synthetic_train Loss: 0.8899 Acc: 0.5664\n",
      "test Loss: 0.7116 Acc: 0.6797\n",
      "Epoch 7/14\n",
      "----------\n",
      "synthetic_train Loss: 0.8431 Acc: 0.6047\n",
      "test Loss: 0.6968 Acc: 0.7063\n",
      "Epoch 8/14\n",
      "----------\n",
      "synthetic_train Loss: 0.8055 Acc: 0.6189\n",
      "test Loss: 0.6068 Acc: 0.7414\n",
      "Epoch 9/14\n",
      "----------\n",
      "synthetic_train Loss: 0.7575 Acc: 0.6455\n",
      "test Loss: 0.5184 Acc: 0.7984\n",
      "Epoch 10/14\n",
      "----------\n",
      "synthetic_train Loss: 0.7333 Acc: 0.6685\n",
      "test Loss: 0.5365 Acc: 0.7914\n",
      "Epoch 11/14\n",
      "----------\n",
      "synthetic_train Loss: 0.6920 Acc: 0.6763\n",
      "test Loss: 0.5328 Acc: 0.7906\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 12/14\n",
      "----------\n",
      "synthetic_train Loss: 0.6518 Acc: 0.7117\n",
      "test Loss: 0.3891 Acc: 0.8648\n",
      "Epoch 13/14\n",
      "----------\n",
      "synthetic_train Loss: 0.6259 Acc: 0.7131\n",
      "test Loss: 0.4799 Acc: 0.8297\n",
      "Epoch 14/14\n",
      "----------\n",
      "synthetic_train Loss: 0.5914 Acc: 0.7319\n",
      "test Loss: 0.3691 Acc: 0.8867\n",
      "\n",
      "Training complete in 2m 47s\n",
      "Best test Loss: 0.369096\n",
      "Epoch 0/14\n",
      "----------\n",
      "synthetic_train Loss: 1.0616 Acc: 0.4871\n",
      "test Loss: 1.0415 Acc: 0.5305\n",
      "Epoch 1/14\n",
      "----------\n",
      "synthetic_train Loss: 1.0132 Acc: 0.5132\n",
      "test Loss: 0.9310 Acc: 0.5383\n",
      "Epoch 2/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9657 Acc: 0.5344\n",
      "test Loss: 0.9161 Acc: 0.5602\n",
      "Epoch 3/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9185 Acc: 0.5618\n",
      "test Loss: 0.8485 Acc: 0.6016\n",
      "Epoch 4/14\n",
      "----------\n",
      "synthetic_train Loss: 0.8537 Acc: 0.5869\n",
      "test Loss: 0.7240 Acc: 0.6758\n",
      "Epoch 5/14\n",
      "----------\n",
      "synthetic_train Loss: 0.7841 Acc: 0.6423\n",
      "test Loss: 0.6234 Acc: 0.7422\n",
      "Epoch 6/14\n",
      "----------\n",
      "synthetic_train Loss: 0.7060 Acc: 0.6855\n",
      "test Loss: 0.4252 Acc: 0.8281\n",
      "Epoch 7/14\n",
      "----------\n",
      "synthetic_train Loss: 0.6284 Acc: 0.7229\n",
      "test Loss: 0.3371 Acc: 0.8594\n",
      "Epoch 8/14\n",
      "----------\n",
      "synthetic_train Loss: 0.5865 Acc: 0.7405\n",
      "test Loss: 0.3079 Acc: 0.8820\n",
      "Epoch 9/14\n",
      "----------\n",
      "synthetic_train Loss: 0.5574 Acc: 0.7546\n",
      "test Loss: 0.3694 Acc: 0.8625\n",
      "Epoch 10/14\n",
      "----------\n",
      "synthetic_train Loss: 0.5095 Acc: 0.7751\n",
      "test Loss: 0.2514 Acc: 0.9094\n",
      "Epoch 11/14\n",
      "----------\n",
      "synthetic_train Loss: 0.4665 Acc: 0.7930\n",
      "test Loss: 0.3036 Acc: 0.8953\n",
      "Epoch 12/14\n",
      "----------\n",
      "synthetic_train Loss: 0.4409 Acc: 0.8042\n",
      "test Loss: 0.2983 Acc: 0.9094\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 13/14\n",
      "----------\n",
      "synthetic_train Loss: 0.4236 Acc: 0.8196\n",
      "test Loss: 0.2249 Acc: 0.9273\n",
      "Epoch 14/14\n",
      "----------\n",
      "synthetic_train Loss: 0.3880 Acc: 0.8293\n",
      "test Loss: 0.2369 Acc: 0.9219\n",
      "\n",
      "Training complete in 2m 37s\n",
      "Best test Loss: 0.224908\n",
      "Epoch 0/14\n",
      "----------\n",
      "synthetic_train Loss: 1.0593 Acc: 0.4907\n",
      "test Loss: 0.9544 Acc: 0.5078\n",
      "Epoch 1/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9925 Acc: 0.5154\n",
      "test Loss: 0.8838 Acc: 0.5828\n",
      "Epoch 2/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9677 Acc: 0.5288\n",
      "test Loss: 0.8492 Acc: 0.6070\n",
      "Epoch 3/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9183 Acc: 0.5569\n",
      "test Loss: 0.8397 Acc: 0.6109\n",
      "Epoch 4/14\n",
      "----------\n",
      "synthetic_train Loss: 0.8954 Acc: 0.5791\n",
      "test Loss: 0.6647 Acc: 0.7180\n",
      "Epoch 5/14\n",
      "----------\n",
      "synthetic_train Loss: 0.8434 Acc: 0.6006\n",
      "test Loss: 0.8316 Acc: 0.6273\n",
      "Epoch 6/14\n",
      "----------\n",
      "synthetic_train Loss: 0.8045 Acc: 0.6235\n",
      "test Loss: 0.5864 Acc: 0.7570\n",
      "Epoch 7/14\n",
      "----------\n",
      "synthetic_train Loss: 0.7487 Acc: 0.6538\n",
      "test Loss: 0.4418 Acc: 0.8234\n",
      "Epoch 8/14\n",
      "----------\n",
      "synthetic_train Loss: 0.7075 Acc: 0.6743\n",
      "test Loss: 0.4201 Acc: 0.8461\n",
      "Epoch 9/14\n",
      "----------\n",
      "synthetic_train Loss: 0.6595 Acc: 0.7007\n",
      "test Loss: 0.3325 Acc: 0.8867\n",
      "Epoch 10/14\n",
      "----------\n",
      "synthetic_train Loss: 0.6517 Acc: 0.7056\n",
      "test Loss: 0.3263 Acc: 0.8844\n",
      "Epoch 11/14\n",
      "----------\n",
      "synthetic_train Loss: 0.6166 Acc: 0.7163\n",
      "test Loss: 0.2380 Acc: 0.9156\n",
      "Epoch 12/14\n",
      "----------\n",
      "synthetic_train Loss: 0.5745 Acc: 0.7449\n",
      "test Loss: 0.3755 Acc: 0.8656\n",
      "Epoch 13/14\n",
      "----------\n",
      "synthetic_train Loss: 0.5648 Acc: 0.7388\n",
      "test Loss: 0.2373 Acc: 0.9234\n",
      "Epoch 14/14\n",
      "----------\n",
      "synthetic_train Loss: 0.5305 Acc: 0.7615\n",
      "test Loss: 0.3159 Acc: 0.8914\n",
      "\n",
      "Training complete in 2m 50s\n",
      "Best test Loss: 0.237257\n",
      "Epoch 0/14\n",
      "----------\n",
      "synthetic_train Loss: 1.0537 Acc: 0.4976\n",
      "test Loss: 0.9760 Acc: 0.5094\n",
      "Epoch 1/14\n",
      "----------\n",
      "synthetic_train Loss: 1.0017 Acc: 0.5107\n",
      "test Loss: 0.8986 Acc: 0.5672\n",
      "Epoch 2/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9691 Acc: 0.5364\n",
      "test Loss: 0.8992 Acc: 0.5766\n",
      "Epoch 3/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9544 Acc: 0.5444\n",
      "test Loss: 0.8605 Acc: 0.5883\n",
      "Epoch 4/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9197 Acc: 0.5632\n",
      "test Loss: 0.8411 Acc: 0.6023\n",
      "Epoch 5/14\n",
      "----------\n",
      "synthetic_train Loss: 0.8797 Acc: 0.5835\n",
      "test Loss: 0.7474 Acc: 0.6414\n",
      "Epoch 6/14\n",
      "----------\n",
      "synthetic_train Loss: 0.8532 Acc: 0.5959\n",
      "test Loss: 0.7224 Acc: 0.6781\n",
      "Epoch 7/14\n",
      "----------\n",
      "synthetic_train Loss: 0.7959 Acc: 0.6428\n",
      "test Loss: 0.5688 Acc: 0.7594\n",
      "Epoch 8/14\n",
      "----------\n",
      "synthetic_train Loss: 0.7525 Acc: 0.6580\n",
      "test Loss: 0.5802 Acc: 0.7586\n",
      "Epoch 9/14\n",
      "----------\n",
      "synthetic_train Loss: 0.7231 Acc: 0.6819\n",
      "test Loss: 0.5130 Acc: 0.7844\n",
      "Epoch 10/14\n",
      "----------\n",
      "synthetic_train Loss: 0.6781 Acc: 0.6904\n",
      "test Loss: 0.5013 Acc: 0.8016\n",
      "Epoch 11/14\n",
      "----------\n",
      "synthetic_train Loss: 0.6361 Acc: 0.7102\n",
      "test Loss: 0.3897 Acc: 0.8461\n",
      "Epoch 12/14\n",
      "----------\n",
      "synthetic_train Loss: 0.5967 Acc: 0.7358\n",
      "test Loss: 0.3186 Acc: 0.8797\n",
      "Epoch 13/14\n",
      "----------\n",
      "synthetic_train Loss: 0.5735 Acc: 0.7478\n",
      "test Loss: 0.3525 Acc: 0.8828\n",
      "Epoch 14/14\n",
      "----------\n",
      "synthetic_train Loss: 0.5161 Acc: 0.7722\n",
      "test Loss: 0.3288 Acc: 0.8930\n",
      "Stopping early due to no improvement in validation loss.\n",
      "\n",
      "Training complete in 2m 36s\n",
      "Best test Loss: 0.318593\n",
      "Epoch 0/14\n",
      "----------\n",
      "synthetic_train Loss: 1.0632 Acc: 0.4849\n",
      "test Loss: 1.0758 Acc: 0.3609\n",
      "Epoch 1/14\n",
      "----------\n",
      "synthetic_train Loss: 1.0183 Acc: 0.5051\n",
      "test Loss: 0.9315 Acc: 0.5211\n",
      "Epoch 2/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9705 Acc: 0.5349\n",
      "test Loss: 0.8597 Acc: 0.5875\n",
      "Epoch 3/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9409 Acc: 0.5479\n",
      "test Loss: 0.8275 Acc: 0.5820\n",
      "Epoch 4/14\n",
      "----------\n",
      "synthetic_train Loss: 0.8862 Acc: 0.5693\n",
      "test Loss: 0.7418 Acc: 0.6617\n",
      "Epoch 5/14\n",
      "----------\n",
      "synthetic_train Loss: 0.8251 Acc: 0.6130\n",
      "test Loss: 0.7114 Acc: 0.6742\n",
      "Epoch 6/14\n",
      "----------\n",
      "synthetic_train Loss: 0.7514 Acc: 0.6482\n",
      "test Loss: 0.5905 Acc: 0.7570\n",
      "Epoch 7/14\n",
      "----------\n",
      "synthetic_train Loss: 0.6769 Acc: 0.6887\n",
      "test Loss: 0.4122 Acc: 0.8500\n",
      "Epoch 8/14\n",
      "----------\n",
      "synthetic_train Loss: 0.6090 Acc: 0.7244\n",
      "test Loss: 0.3783 Acc: 0.8547\n",
      "Epoch 9/14\n",
      "----------\n",
      "synthetic_train Loss: 0.5626 Acc: 0.7522\n",
      "test Loss: 0.3589 Acc: 0.8602\n",
      "Epoch 10/14\n",
      "----------\n",
      "synthetic_train Loss: 0.5244 Acc: 0.7649\n",
      "test Loss: 0.2670 Acc: 0.9078\n",
      "Epoch 11/14\n",
      "----------\n",
      "synthetic_train Loss: 0.4865 Acc: 0.7903\n",
      "test Loss: 0.2949 Acc: 0.9031\n",
      "Epoch 12/14\n",
      "----------\n",
      "synthetic_train Loss: 0.4374 Acc: 0.7991\n",
      "test Loss: 0.2590 Acc: 0.9195\n",
      "Epoch 13/14\n",
      "----------\n",
      "synthetic_train Loss: 0.4351 Acc: 0.8103\n",
      "test Loss: 0.2552 Acc: 0.9258\n",
      "Epoch 14/14\n",
      "----------\n",
      "synthetic_train Loss: 0.3931 Acc: 0.8274\n",
      "test Loss: 0.2528 Acc: 0.9258\n",
      "\n",
      "Training complete in 2m 48s\n",
      "Best test Loss: 0.252790\n",
      "Epoch 0/14\n",
      "----------\n",
      "synthetic_train Loss: 1.0665 Acc: 0.4907\n",
      "test Loss: 1.0249 Acc: 0.4953\n",
      "Epoch 1/14\n",
      "----------\n",
      "synthetic_train Loss: 1.0182 Acc: 0.4988\n",
      "test Loss: 1.0285 Acc: 0.4977\n",
      "Epoch 2/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9771 Acc: 0.5259\n",
      "test Loss: 0.9237 Acc: 0.5148\n",
      "Epoch 3/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9420 Acc: 0.5422\n",
      "test Loss: 0.8570 Acc: 0.5836\n",
      "Epoch 4/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9053 Acc: 0.5571\n",
      "test Loss: 0.8034 Acc: 0.6133\n",
      "Epoch 5/14\n",
      "----------\n",
      "synthetic_train Loss: 0.8640 Acc: 0.5979\n",
      "test Loss: 0.7321 Acc: 0.6703\n",
      "Epoch 6/14\n",
      "----------\n",
      "synthetic_train Loss: 0.7800 Acc: 0.6389\n",
      "test Loss: 0.5428 Acc: 0.7672\n",
      "Epoch 7/14\n",
      "----------\n",
      "synthetic_train Loss: 0.7158 Acc: 0.6687\n",
      "test Loss: 0.6181 Acc: 0.7516\n",
      "Epoch 8/14\n",
      "----------\n",
      "synthetic_train Loss: 0.6797 Acc: 0.6897\n",
      "test Loss: 0.4139 Acc: 0.8273\n",
      "Epoch 9/14\n",
      "----------\n",
      "synthetic_train Loss: 0.6243 Acc: 0.7209\n",
      "test Loss: 0.3493 Acc: 0.8711\n",
      "Epoch 10/14\n",
      "----------\n",
      "synthetic_train Loss: 0.5768 Acc: 0.7422\n",
      "test Loss: 0.3912 Acc: 0.8617\n",
      "Epoch 11/14\n",
      "----------\n",
      "synthetic_train Loss: 0.5479 Acc: 0.7556\n",
      "test Loss: 0.2844 Acc: 0.8875\n",
      "Epoch 12/14\n",
      "----------\n",
      "synthetic_train Loss: 0.4996 Acc: 0.7751\n",
      "test Loss: 0.4237 Acc: 0.8555\n",
      "Epoch 13/14\n",
      "----------\n",
      "synthetic_train Loss: 0.4821 Acc: 0.7900\n",
      "test Loss: 0.4483 Acc: 0.8641\n",
      "Stopping early due to no improvement in validation loss.\n",
      "Epoch 14/14\n",
      "----------\n",
      "synthetic_train Loss: 0.4438 Acc: 0.8037\n",
      "test Loss: 0.2812 Acc: 0.9016\n",
      "\n",
      "Training complete in 2m 32s\n",
      "Best test Loss: 0.281163\n",
      "Epoch 0/14\n",
      "----------\n",
      "synthetic_train Loss: 1.0525 Acc: 0.5020\n",
      "test Loss: 1.0118 Acc: 0.4953\n",
      "Epoch 1/14\n",
      "----------\n",
      "synthetic_train Loss: 1.0151 Acc: 0.5078\n",
      "test Loss: 0.8968 Acc: 0.5625\n",
      "Epoch 2/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9615 Acc: 0.5295\n",
      "test Loss: 0.8706 Acc: 0.5828\n",
      "Epoch 3/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9091 Acc: 0.5576\n",
      "test Loss: 0.8124 Acc: 0.6273\n",
      "Epoch 4/14\n",
      "----------\n",
      "synthetic_train Loss: 0.8727 Acc: 0.5942\n",
      "test Loss: 0.7747 Acc: 0.6742\n",
      "Epoch 5/14\n",
      "----------\n",
      "synthetic_train Loss: 0.7945 Acc: 0.6323\n",
      "test Loss: 0.7290 Acc: 0.7219\n",
      "Epoch 6/14\n",
      "----------\n",
      "synthetic_train Loss: 0.7220 Acc: 0.6738\n",
      "test Loss: 0.6492 Acc: 0.7289\n",
      "Epoch 7/14\n",
      "----------\n",
      "synthetic_train Loss: 0.6749 Acc: 0.6990\n",
      "test Loss: 0.4611 Acc: 0.8180\n",
      "Epoch 8/14\n",
      "----------\n",
      "synthetic_train Loss: 0.6249 Acc: 0.7241\n",
      "test Loss: 0.5867 Acc: 0.8086\n",
      "Epoch 9/14\n",
      "----------\n",
      "synthetic_train Loss: 0.5818 Acc: 0.7422\n",
      "test Loss: 0.3968 Acc: 0.8656\n",
      "Epoch 10/14\n",
      "----------\n",
      "synthetic_train Loss: 0.5356 Acc: 0.7573\n",
      "test Loss: 0.3180 Acc: 0.8859\n",
      "Epoch 11/14\n",
      "----------\n",
      "synthetic_train Loss: 0.4974 Acc: 0.7766\n",
      "test Loss: 0.3075 Acc: 0.9031\n",
      "Epoch 12/14\n",
      "----------\n",
      "synthetic_train Loss: 0.4679 Acc: 0.7961\n",
      "test Loss: 0.3182 Acc: 0.8758\n",
      "Epoch 13/14\n",
      "----------\n",
      "synthetic_train Loss: 0.4246 Acc: 0.8076\n",
      "test Loss: 0.2396 Acc: 0.9125\n",
      "Epoch 14/14\n",
      "----------\n",
      "synthetic_train Loss: 0.4241 Acc: 0.8115\n",
      "test Loss: 0.2116 Acc: 0.9273\n",
      "\n",
      "Training complete in 2m 36s\n",
      "Best test Loss: 0.211568\n",
      "Epoch 0/14\n",
      "----------\n",
      "synthetic_train Loss: 1.0544 Acc: 0.4866\n",
      "test Loss: 1.0079 Acc: 0.5094\n",
      "Epoch 1/14\n",
      "----------\n",
      "synthetic_train Loss: 1.0167 Acc: 0.5090\n",
      "test Loss: 0.9331 Acc: 0.5602\n",
      "Epoch 2/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9990 Acc: 0.5134\n",
      "test Loss: 0.9076 Acc: 0.5586\n",
      "Epoch 3/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9731 Acc: 0.5232\n",
      "test Loss: 0.8718 Acc: 0.5609\n",
      "Epoch 4/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9339 Acc: 0.5391\n",
      "test Loss: 0.8207 Acc: 0.5914\n",
      "Epoch 5/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9033 Acc: 0.5674\n",
      "test Loss: 0.8288 Acc: 0.6125\n",
      "Epoch 6/14\n",
      "----------\n",
      "synthetic_train Loss: 0.8496 Acc: 0.5930\n",
      "test Loss: 0.7189 Acc: 0.6359\n",
      "Epoch 7/14\n",
      "----------\n",
      "synthetic_train Loss: 0.8019 Acc: 0.6272\n",
      "test Loss: 0.6681 Acc: 0.6937\n",
      "Epoch 8/14\n",
      "----------\n",
      "synthetic_train Loss: 0.7493 Acc: 0.6580\n",
      "test Loss: 0.6538 Acc: 0.7453\n",
      "Epoch 9/14\n",
      "----------\n",
      "synthetic_train Loss: 0.6760 Acc: 0.6995\n",
      "test Loss: 0.4219 Acc: 0.8281\n",
      "Epoch 10/14\n",
      "----------\n",
      "synthetic_train Loss: 0.6402 Acc: 0.7124\n",
      "test Loss: 0.4513 Acc: 0.8203\n",
      "Epoch 11/14\n",
      "----------\n",
      "synthetic_train Loss: 0.5771 Acc: 0.7512\n",
      "test Loss: 0.3217 Acc: 0.8773\n",
      "Epoch 12/14\n",
      "----------\n",
      "synthetic_train Loss: 0.5306 Acc: 0.7681\n",
      "test Loss: 0.3045 Acc: 0.8805\n",
      "Epoch 13/14\n",
      "----------\n",
      "synthetic_train Loss: 0.4789 Acc: 0.7930\n",
      "test Loss: 0.2373 Acc: 0.9141\n",
      "Epoch 14/14\n",
      "----------\n",
      "synthetic_train Loss: 0.4763 Acc: 0.7957\n",
      "test Loss: 0.2117 Acc: 0.9234\n",
      "\n",
      "Training complete in 2m 56s\n",
      "Best test Loss: 0.211658\n",
      "Epoch 0/14\n",
      "----------\n",
      "synthetic_train Loss: 1.0667 Acc: 0.4873\n",
      "test Loss: 1.0060 Acc: 0.4953\n",
      "Epoch 1/14\n",
      "----------\n",
      "synthetic_train Loss: 1.0173 Acc: 0.5115\n",
      "test Loss: 0.9367 Acc: 0.5148\n",
      "Epoch 2/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9802 Acc: 0.5256\n",
      "test Loss: 0.9105 Acc: 0.5484\n",
      "Epoch 3/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9760 Acc: 0.5339\n",
      "test Loss: 0.8369 Acc: 0.5906\n",
      "Epoch 4/14\n",
      "----------\n",
      "synthetic_train Loss: 0.9251 Acc: 0.5574\n",
      "test Loss: 0.7984 Acc: 0.6227\n",
      "Epoch 5/14\n",
      "----------\n",
      "synthetic_train Loss: 0.8889 Acc: 0.5784\n",
      "test Loss: 0.6961 Acc: 0.6891\n",
      "Epoch 6/14\n",
      "----------\n",
      "synthetic_train Loss: 0.8307 Acc: 0.6155\n",
      "test Loss: 0.6732 Acc: 0.7367\n",
      "Epoch 7/14\n",
      "----------\n",
      "synthetic_train Loss: 0.7843 Acc: 0.6367\n",
      "test Loss: 0.5563 Acc: 0.7844\n",
      "Epoch 8/14\n",
      "----------\n",
      "synthetic_train Loss: 0.7377 Acc: 0.6643\n",
      "test Loss: 0.5589 Acc: 0.7523\n",
      "Epoch 9/14\n",
      "----------\n",
      "synthetic_train Loss: 0.6980 Acc: 0.6821\n",
      "test Loss: 0.4193 Acc: 0.8477\n",
      "Epoch 10/14\n",
      "----------\n",
      "synthetic_train Loss: 0.6356 Acc: 0.7100\n",
      "test Loss: 0.4512 Acc: 0.8320\n",
      "Epoch 11/14\n",
      "----------\n",
      "synthetic_train Loss: 0.6156 Acc: 0.7263\n",
      "test Loss: 0.3872 Acc: 0.8617\n",
      "Epoch 12/14\n",
      "----------\n",
      "synthetic_train Loss: 0.5793 Acc: 0.7444\n",
      "test Loss: 0.3064 Acc: 0.8961\n",
      "Epoch 13/14\n",
      "----------\n",
      "synthetic_train Loss: 0.5413 Acc: 0.7573\n",
      "test Loss: 0.3814 Acc: 0.8719\n",
      "Epoch 14/14\n",
      "----------\n",
      "synthetic_train Loss: 0.5016 Acc: 0.7703\n",
      "test Loss: 0.3190 Acc: 0.9164\n",
      "Stopping early due to no improvement in validation loss.\n",
      "\n",
      "Training complete in 2m 45s\n",
      "Best test Loss: 0.306384\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "num_sims = 20\n",
    "\n",
    "train_percentage_dict = {\n",
    "    'NonDemented': 0.8,\n",
    "    'VeryMildDemented':0.8,\n",
    "    'MildDemented':0.8,\n",
    "    'ModerateDemented':0.8,\n",
    "}\n",
    "\n",
    "synth_percentage_dict = {\n",
    "    'NonDemented': 0.0,\n",
    "    'VeryMildDemented':0.0,\n",
    "    'MildDemented':0.0,\n",
    "    'ModerateDemented':0.0,\n",
    "}\n",
    "\n",
    "transforms = {\n",
    "    #'minimal': minimal_transforms,\n",
    "    #'basic': basic_transforms,\n",
    "    'auto': auto_transforms\n",
    "}\n",
    "\n",
    "for active_transform in transforms.keys():\n",
    "    df_all_results = pd.DataFrame()\n",
    "    for n in range(num_sims):\n",
    "\n",
    "        # make the synthetic training dataset\n",
    "        makeSyntheticTrain(\n",
    "            train_directory='../data/alzheimer_mri/train',\n",
    "            synthetic_train_directory='../data/alzheimer_mri/synthetic_train', \n",
    "            train_dict=train_percentage_dict, \n",
    "            synthetic_dict=synth_percentage_dict\n",
    "        )\n",
    "\n",
    "        # get and load datasets\n",
    "        data_dir = '../data/alzheimer_mri'\n",
    "        data_sets = ['synthetic_train','test']\n",
    "        image_datasets, dataloaders, dataset_sizes, class_names, device = get_data(\n",
    "            data_dir=data_dir, \n",
    "            data_sets=data_sets, \n",
    "            data_transforms=transforms.get(active_transform)\n",
    "        )\n",
    "\n",
    "        # instantiate model\n",
    "        model = cnn.CNN(in_channels=1, num_classes=4)\n",
    "\n",
    "        # Set the size of each output sample to nn.Linear(num_ftrs, len(class_names))\n",
    "        #num_ftrs = 4 #model.fc.in_features\n",
    "        #model.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "        model = model.to(device)\n",
    "\n",
    "        # train the model\n",
    "        df_results,_ = model = train_model(\n",
    "            model=model, \n",
    "            criterion = nn.CrossEntropyLoss(),\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.001), \n",
    "            dataloaders=dataloaders,\n",
    "            dataset_sizes=dataset_sizes,\n",
    "            num_epochs=num_epochs\n",
    "        )\n",
    "\n",
    "        #df_results['train_percentage'] = train_percentage\n",
    "        #df_results['synth_percentage'] = synthetic_percentage\n",
    "        df_results['train_synth_ratio'] = '__'.join([k+str(v1)+'_'+str(v2) for k,v1,v2 in zip(train_percentage_dict.keys(),train_percentage_dict.values(),synth_percentage_dict.values())])\n",
    "        df_results['transform'] = active_transform\n",
    "        df_results['sim_num'] = n\n",
    "        df_results['category'] = df_results.apply(lambda row: row['transform']+'_'+'__'.join([k+str(v1)+'_'+str(v2) for k,v1,v2 in zip(train_percentage_dict.keys(),train_percentage_dict.values(),synth_percentage_dict.values())]), axis=1)\n",
    "        df_all_results = pd.concat([df_all_results, df_results],ignore_index=True)\n",
    "    df_all_results.to_csv(f'../results/results_cnn_{active_transform}' + '__'.join([k+str(v1)+'_'+str(v2) for k,v1,v2 in zip(train_percentage_dict.keys(),train_percentage_dict.values(),synth_percentage_dict.values())]) + '.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
